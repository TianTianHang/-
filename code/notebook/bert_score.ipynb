{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(inputs: tuple) -> str:\n",
    "    phrase =inputs[0]\n",
    "    text = inputs[1]\n",
    "    masked_phrase = f\"[MASK]\"\n",
    "    text = re.sub(rf'\\b{phrase}\\b', masked_phrase, text, flags=re.IGNORECASE)\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T05:14:41.790885600Z",
     "start_time": "2024-01-28T05:14:41.785791100Z"
    }
   },
   "id": "ad5781aa2b657da2",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-28T05:16:24.734038900Z",
     "start_time": "2024-01-28T05:16:22.311421500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "' Under Doug McMillon\\'s bold leadership, Walmart here again is taking one of the most essential steps to long-term success for any legacy bricks-and-mortar retailer—specifically, redefining how it thinks of the word \"product.\"\\nInstead of doing what many retailers do, like staffing their \"product\" heads within owned brand development, store operations, e-commerce, etc. and then leaving them to collaborate inside complex organizations with competing priorities, Walmart rightly understands that retailing has become so complex that all these aforementioned roles now need their own steward to ensure alignment.'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "masker = pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=\"bert-base-uncased\",\n",
    "    tokenizer=\"bert-base-uncased\",\n",
    ")\n",
    "inputs=(\n",
    "            \"de facto\",\" Under Doug McMillon's bold leadership, Walmart here again is taking one of the most essential steps to long-term success for any legacy bricks-and-mortar retailer—specifically, redefining how it thinks of the word \\\"product.\\\"\\nInstead of doing what many retailers do, like staffing de facto \\\"product\\\" heads within owned brand development, store operations, e-commerce, etc. and then leaving them to collaborate inside complex organizations with competing priorities, Walmart rightly understands that retailing has become so complex that all these aforementioned roles now need their own steward to ensure alignment.\"\n",
    ")\n",
    "\n",
    "text_with_blank=preprocess_text(inputs)\n",
    "predictions =masker(text_with_blank)\n",
    "filled_text = text_with_blank.replace('[MASK]', predictions[0]['token_str'])\n",
    "filled_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
