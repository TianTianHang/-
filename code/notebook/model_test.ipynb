{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "'cuda:0'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T02:19:53.626118200Z",
     "start_time": "2024-02-05T02:19:43.864267600Z"
    }
   },
   "id": "373777f75ca5a1b3",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义参数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffde2b3ef49130d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name=\"facebook/nllb-200-distilled-1.3B\"\n",
    "source_lang=\"en\"\n",
    "target_lang=\"de\"\n",
    "dataset_name=\"wmt16\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T05:51:34.583287600Z",
     "start_time": "2024-02-05T05:51:34.577976Z"
    }
   },
   "id": "809ab033b1652a51",
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "source": [
    "下载读取WMT数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e96562848f5ff1bd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt16 (C:/Users/tiantian/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "816085d7ffd24fdcaa9bb776766aba7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download the WMT dataset\n",
    "dataset = load_dataset(dataset_name, f\"de-en\")\n",
    "translation=dataset[\"test\"][\"translation\"][:100]\n",
    "source_texts = [t[source_lang] for t in translation]  # Adjust the size as needed\n",
    "reference_texts = [t[target_lang] for t in translation]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T04:31:05.057330800Z",
     "start_time": "2024-02-05T04:30:58.775207700Z"
    }
   },
   "id": "ee7e9c71f47a8d87",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2859169c5dd10afe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[63], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[0;32m      2\u001B[0m lang_piece\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(src_lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meng_Latn\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      3\u001B[0m     tgt_lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdeu_Latn\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m translation_pipeline \u001B[38;5;241m=\u001B[39m pipeline(task\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranslation\u001B[39m\u001B[38;5;124m\"\u001B[39m, model\u001B[38;5;241m=\u001B[39mmodel_name,device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m      5\u001B[0m translation_pipeline(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhello world\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mlang_piece)\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:807\u001B[0m, in \u001B[0;36mpipeline\u001B[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[0;32m    805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m framework \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    806\u001B[0m     model_classes \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m: targeted_task[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m: targeted_task[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m]}\n\u001B[1;32m--> 807\u001B[0m     framework, model \u001B[38;5;241m=\u001B[39m infer_framework_load_model(\n\u001B[0;32m    808\u001B[0m         model,\n\u001B[0;32m    809\u001B[0m         model_classes\u001B[38;5;241m=\u001B[39mmodel_classes,\n\u001B[0;32m    810\u001B[0m         config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[0;32m    811\u001B[0m         framework\u001B[38;5;241m=\u001B[39mframework,\n\u001B[0;32m    812\u001B[0m         task\u001B[38;5;241m=\u001B[39mtask,\n\u001B[0;32m    813\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs,\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m    815\u001B[0m     )\n\u001B[0;32m    817\u001B[0m model_config \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\n\u001B[0;32m    818\u001B[0m hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_commit_hash\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:267\u001B[0m, in \u001B[0;36minfer_framework_load_model\u001B[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001B[0m\n\u001B[0;32m    261\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    262\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    263\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrying to load the model with Tensorflow.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    264\u001B[0m     )\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 267\u001B[0m     model \u001B[38;5;241m=\u001B[39m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(model, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    268\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    269\u001B[0m         model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:516\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    514\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    515\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    517\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    518\u001B[0m     )\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    521\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    522\u001B[0m )\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3091\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   3081\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3082\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[0;32m   3084\u001B[0m     (\n\u001B[0;32m   3085\u001B[0m         model,\n\u001B[0;32m   3086\u001B[0m         missing_keys,\n\u001B[0;32m   3087\u001B[0m         unexpected_keys,\n\u001B[0;32m   3088\u001B[0m         mismatched_keys,\n\u001B[0;32m   3089\u001B[0m         offload_index,\n\u001B[0;32m   3090\u001B[0m         error_msgs,\n\u001B[1;32m-> 3091\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_load_pretrained_model(\n\u001B[0;32m   3092\u001B[0m         model,\n\u001B[0;32m   3093\u001B[0m         state_dict,\n\u001B[0;32m   3094\u001B[0m         loaded_state_dict_keys,  \u001B[38;5;66;03m# XXX: rename?\u001B[39;00m\n\u001B[0;32m   3095\u001B[0m         resolved_archive_file,\n\u001B[0;32m   3096\u001B[0m         pretrained_model_name_or_path,\n\u001B[0;32m   3097\u001B[0m         ignore_mismatched_sizes\u001B[38;5;241m=\u001B[39mignore_mismatched_sizes,\n\u001B[0;32m   3098\u001B[0m         sharded_metadata\u001B[38;5;241m=\u001B[39msharded_metadata,\n\u001B[0;32m   3099\u001B[0m         _fast_init\u001B[38;5;241m=\u001B[39m_fast_init,\n\u001B[0;32m   3100\u001B[0m         low_cpu_mem_usage\u001B[38;5;241m=\u001B[39mlow_cpu_mem_usage,\n\u001B[0;32m   3101\u001B[0m         device_map\u001B[38;5;241m=\u001B[39mdevice_map,\n\u001B[0;32m   3102\u001B[0m         offload_folder\u001B[38;5;241m=\u001B[39moffload_folder,\n\u001B[0;32m   3103\u001B[0m         offload_state_dict\u001B[38;5;241m=\u001B[39moffload_state_dict,\n\u001B[0;32m   3104\u001B[0m         dtype\u001B[38;5;241m=\u001B[39mtorch_dtype,\n\u001B[0;32m   3105\u001B[0m         is_quantized\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_method\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m QuantizationMethod\u001B[38;5;241m.\u001B[39mBITS_AND_BYTES),\n\u001B[0;32m   3106\u001B[0m         keep_in_fp32_modules\u001B[38;5;241m=\u001B[39mkeep_in_fp32_modules,\n\u001B[0;32m   3107\u001B[0m     )\n\u001B[0;32m   3109\u001B[0m model\u001B[38;5;241m.\u001B[39mis_loaded_in_4bit \u001B[38;5;241m=\u001B[39m load_in_4bit\n\u001B[0;32m   3110\u001B[0m model\u001B[38;5;241m.\u001B[39mis_loaded_in_8bit \u001B[38;5;241m=\u001B[39m load_in_8bit\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3424\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001B[0m\n\u001B[0;32m   3414\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m state_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3415\u001B[0m     \u001B[38;5;66;03m# Whole checkpoint\u001B[39;00m\n\u001B[0;32m   3416\u001B[0m     mismatched_keys \u001B[38;5;241m=\u001B[39m _find_mismatched_keys(\n\u001B[0;32m   3417\u001B[0m         state_dict,\n\u001B[0;32m   3418\u001B[0m         model_state_dict,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3422\u001B[0m         ignore_mismatched_sizes,\n\u001B[0;32m   3423\u001B[0m     )\n\u001B[1;32m-> 3424\u001B[0m     error_msgs \u001B[38;5;241m=\u001B[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001B[0;32m   3425\u001B[0m     offload_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   3426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3427\u001B[0m     \u001B[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001B[39;00m\n\u001B[0;32m   3428\u001B[0m \n\u001B[0;32m   3429\u001B[0m     \u001B[38;5;66;03m# This should always be a list but, just to be sure.\u001B[39;00m\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:566\u001B[0m, in \u001B[0;36m_load_state_dict_into_model\u001B[1;34m(model_to_load, state_dict, start_prefix)\u001B[0m\n\u001B[0;32m    563\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    564\u001B[0m             load(child, state_dict, prefix \u001B[38;5;241m+\u001B[39m name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 566\u001B[0m load(model_to_load, state_dict, prefix\u001B[38;5;241m=\u001B[39mstart_prefix)\n\u001B[0;32m    567\u001B[0m \u001B[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001B[39;00m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;66;03m# it's safe to delete it.\u001B[39;00m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m state_dict\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:564\u001B[0m, in \u001B[0;36m_load_state_dict_into_model.<locals>.load\u001B[1;34m(module, state_dict, prefix)\u001B[0m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 564\u001B[0m         load(child, state_dict, prefix \u001B[38;5;241m+\u001B[39m name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:564\u001B[0m, in \u001B[0;36m_load_state_dict_into_model.<locals>.load\u001B[1;34m(module, state_dict, prefix)\u001B[0m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 564\u001B[0m         load(child, state_dict, prefix \u001B[38;5;241m+\u001B[39m name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "    \u001B[1;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 564 (3 times)]\u001B[0m\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:564\u001B[0m, in \u001B[0;36m_load_state_dict_into_model.<locals>.load\u001B[1;34m(module, state_dict, prefix)\u001B[0m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 564\u001B[0m         load(child, state_dict, prefix \u001B[38;5;241m+\u001B[39m name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:560\u001B[0m, in \u001B[0;36m_load_state_dict_into_model.<locals>.load\u001B[1;34m(module, state_dict, prefix)\u001B[0m\n\u001B[0;32m    558\u001B[0m                     module\u001B[38;5;241m.\u001B[39m_load_from_state_dict(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m    559\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 560\u001B[0m         module\u001B[38;5;241m.\u001B[39m_load_from_state_dict(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, child \u001B[38;5;129;01min\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m child \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\tools\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2040\u001B[0m, in \u001B[0;36mModule._load_from_state_dict\u001B[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001B[0m\n\u001B[0;32m   2038\u001B[0m                 \u001B[38;5;28msetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, input_param)\n\u001B[0;32m   2039\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2040\u001B[0m             param\u001B[38;5;241m.\u001B[39mcopy_(input_param)\n\u001B[0;32m   2041\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m ex:\n\u001B[0;32m   2042\u001B[0m     error_msgs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWhile copying the parameter named \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2043\u001B[0m                       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhose dimensions in the model are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2044\u001B[0m                       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhose dimensions in the checkpoint are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_param\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2045\u001B[0m                       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124man exception occurred : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mex\u001B[38;5;241m.\u001B[39margs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   2046\u001B[0m                       )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "lang_piece=dict(src_lang='eng_Latn',\n",
    "    tgt_lang='deu_Latn')\n",
    "translation_pipeline = pipeline(task=f\"translation\", model=model_name,device=device)\n",
    "translation_pipeline('hello world', **lang_piece)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T06:27:48.839210500Z",
     "start_time": "2024-02-05T06:25:12.807381700Z"
    }
   },
   "id": "faeca7c60c23c66b",
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "生成翻译句子"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee790a5370af3523"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "translations = translation_pipeline(source_texts, **lang_piece)\n",
    "translations = [translation['translation_text'] for translation in translations]\n",
    "# 输出翻译结果\n",
    "df = pd.DataFrame({\n",
    "    'Source': source_texts,\n",
    "    'Translation': translations\n",
    "})\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-05T06:02:47.082482200Z"
    }
   },
   "id": "23e5f69c3cbb9f9c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用sacrebleu评价模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bb0454bef377196"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'BLEU Score: 44.75055716583148'"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sacrebleu\n",
    "bleu_score = sacrebleu.corpus_bleu(translations, [reference_texts]).score\n",
    "f\"BLEU Score: {bleu_score}\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T05:46:32.323102800Z",
     "start_time": "2024-02-05T05:46:32.277039Z"
    }
   },
   "id": "f31f8e7e6a19addd",
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "写入文件"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe8d0e692d351074"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"./BLEU_result.txt\",\"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(f\"\\n{model_name} {source_lang} {target_lang} {bleu_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T05:46:37.197513900Z",
     "start_time": "2024-02-05T05:46:37.186251900Z"
    }
   },
   "id": "1ea7ef64ed08ec44",
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
