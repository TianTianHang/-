# 依赖指标#

<ol><li><p><strong>BLEU（Bilingual Evaluation Understudy）：</strong> BLEU是一种常用的自动评估指标，用于测量翻译结果与参考翻译之间的相似度。该指标的值在0到1之间，越接近1表示翻译质量越好。BLEU考虑了n-gram的匹配以及翻译长度的惩罚。</p></li><li><p><strong>TER（Translation Edit Rate）：</strong> TER是一种衡量翻译质量的指标，它通过计算编辑操作的数量（插入、删除、替换）来评估翻译与参考之间的差异。与BLEU不同，TER越低表示翻译质量越好。</p></li><li><p><strong>METEOR：</strong> METEOR结合了精确度、召回率和一些额外的特征，如同义词的考虑，以生成一个综合的翻译质量分数。METEOR的分数越高表示翻译质量越好。</p></li><li><p><strong>ROUGE（Recall-Oriented Understudy for Gisting Evaluation）：</strong> ROUGE主要用于评估自动摘要质量，但也可以应用于机器翻译。它考虑了n-gram的匹配、召回率和其他特征。ROUGE的分数越高表示翻译质量越好。</p></li><li><p><strong>CIDEr（Consensus-based Image Description Evaluation）：</strong> CIDEr最初是为图像描述生成而设计的，但也可以用于翻译质量的评估。它通过考虑多个参考翻译来提供一种一致性评估。</p></li><li><p><strong>WER（Word Error Rate）：</strong> WER是一种用于评估语音识别和翻译质量的指标。它通过计算插入、删除和替换的操作数量来测量翻译与参考之间的差异。与TER类似，WER越低表示翻译质量越好。</p></li><li><p><strong>Human Evaluation：</strong> 人工评估是评估翻译质量的重要方法之一。通过邀请人类评估者对翻译结果进行评分，可以获得更直观和真实的评估结果。</p></li></ol>

# 无依赖指标 #

## XBERTScore ##

## YiSi-2 ##

## SentSim ##
  
## BERTScore-MKD ##